{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "This script takes an excel file of SEATRAC authors and a date range and searches PubMed for papers from those authors published during those dates. Output is a semicolon-separated list of PubMed IDs which can be pasted directly into the PubMed search box to get a page listing those papers.\n",
    "\n",
    "1. Upload \"20231114_SEATRAC_Member_PubSearch.xlsx\":\n",
    "    * Click the folder symbol on the left, then the upload symbol. If you get a warning about saving elsewhere click \"ok\".\n",
    "    * If you're using a newer version ensure these columns still exist: 'Last Name', 'First Name', 'Middle Initial', 'Primary Institution (Choose ONE)'\n",
    "2. Update the search dates:\n",
    "    * In the \"Inputs\" code box below, edit the text within quotes\n",
    "    * Click the play button to run this chunk of code, saving the inputs\n",
    "3. Do the search:\n",
    "    * Click the play button of the 'Do PubMed Search' code box (should take 1-2min to finish)\n",
    "\n",
    "#### Extra info:\n",
    "\n",
    "* PubMed API (NCBI Entrez E-utilities) help: https://www.ncbi.nlm.nih.gov/books/NBK25497/#chapter2.Introduction\n",
    "* More help: https://pubmed.ncbi.nlm.nih.gov/help/\n",
    "* List of PubMed \"tags\": https://www.ncbi.nlm.nih.gov/pmc/about/userguide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2023/07/01'\n",
    "end_date = '2023/09/30'\n",
    "\n",
    "excel_file = '/content/20231114_SEATRAC_Member_PubSearch.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do PubMed Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading packages and data\n",
      "Formatting affiliations and concepts\n",
      "Formatting author names (searching for recent papers to decide whether to use middle initial)\n",
      "Doing PubMed search\n",
      "PubMed IDs of papers from SEATRAC authors published between 2023/07/01 and 2023/09/30:\n",
      "\n",
      "37773037;37708378;37696247;37693490;37676852;37556423;37461439;37400753;37379655;37336104;36945395\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# Setup #\n",
    "#########\n",
    "\n",
    "print('Loading packages and data')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Create URL prefix Using my NCBI account API key (api_key) and retrieving a \n",
    "# maximum of 10,000 records (retmax)\n",
    "prefix = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&api_key=e8c1951d5b35792885126d8a9597398a1a07&retmax=10000&usehistory=y&term='\n",
    "\n",
    "# Load input excel sheet\n",
    "pi_list = pd.read_excel(excel_file)\n",
    "\n",
    "####################################\n",
    "# Format affiliations and concepts #\n",
    "####################################\n",
    "\n",
    "print('Formatting affiliations and concepts')\n",
    "\n",
    "affiliations_list = pi_list['Primary Institution (Choose ONE)'].drop_duplicates().tolist()\n",
    "affiliations = '(\"' + '\"[ad] OR \"'.join(affiliations_list) + '\")'\n",
    "\n",
    "concept = '(\"tubercul*\"[tw] OR \"Antitubercul*\"[tw] OR \"Anti-Tubercul*\"[tw] OR \"osteotubercul*\"[tw] OR \"nephrotubercul*\"[tw] OR \"anthracosilicotubercul*\"[tw] OR \"coniotubercul*\"[tw] OR \"Tuberculin\"[tw] OR \"tb\"[tw] OR \"xdr-tb\"[tw] OR \"xdrtb\"[tw] OR \"mdr-tb\"[tw] OR \"mdrtb\"[tw] OR \"phthisis\"[tw] OR \"pneumonophthisis\"[tw] OR \"pneumophthisiology\"[tw] OR \"silicotubercul*\"[tw] OR \"bazin disease\"[tw] OR \"erythema induratum\"[tw] OR \"white swelling\"[tw] OR \"king`s evil\"[tw] OR \"scrofula\"[tw] OR \"pott disease\"[tw] OR \"koch`s disease\"[tw] OR \"Interferon-gamma Release Test\"[tw] OR \"Tuberculosis\"[Mesh] OR \"Mycobacterium tuberculosis\"[Mesh] OR \"Antitubercular Agents\"[Mesh] OR \"Tuberculin Test\"[Mesh] OR \"Interferon-gamma Release Tests\"[Mesh] OR \"Tuberculosis Vaccines\"[Mesh])'\n",
    "concept_list = concept.\\\n",
    "    replace('[Mesh]', '[mh]').\\\n",
    "    split(' OR ')\n",
    "concepts = ' OR '.join(concept_list)\n",
    "\n",
    "#######################\n",
    "# Format author names #\n",
    "#######################\n",
    "\n",
    "print('Formatting author names (searching for recent papers to decide whether to use middle initial)')\n",
    "\n",
    "# Step 1: Determine if need to use middle initial\n",
    "\n",
    "pi_list['LastFirst'] = '\"' + pi_list['Last Name'] + ' ' + pi_list['First Name'].str[0] + '\"' + '[au]'\n",
    "pi_list['LastFirstMiddle'] = '\"' + pi_list['Last Name'] + ' ' + pi_list['First Name'].str[0] + pi_list['Middle Initial'].str[0] + '\"' + '[au]'\n",
    "\n",
    "# For name in LastFirst, how many papers show up published within the last four \n",
    "# years? Show authors with no papers (probably need middle initial)\n",
    "pubdate_4yr = '2019/11/21:2023/11/21[pdat]'\n",
    "\n",
    "has_papers = set()\n",
    "no_papers = list()\n",
    "for author in pi_list['LastFirst'].drop_duplicates().to_list():\n",
    "    url = prefix + pubdate_4yr + '+AND+' + affiliations + author\n",
    "    page = requests.get(url).text\n",
    "    result = BeautifulSoup(page, 'xml')\n",
    "    IDlist = [i.text for i in result.find_all('Id')]\n",
    "    num_papers = len(IDlist)\n",
    "    if num_papers > 0:\n",
    "        has_papers.add(author)\n",
    "    else:\n",
    "        no_papers.append(author)\n",
    "\n",
    "# Now look with LastFirstMiddle\n",
    "new_pi_list = pi_list[pi_list['LastFirst'].isin(no_papers)].sort_values(by='LastFirstMiddle')\n",
    "new_has_papers = set()\n",
    "new_no_papers = list()\n",
    "for author in new_pi_list['LastFirstMiddle'].drop_duplicates().dropna().to_list():\n",
    "    url = prefix + pubdate_4yr + '+AND+' + affiliations + author\n",
    "    page = requests.get(url).text\n",
    "    result = BeautifulSoup(page, 'xml')\n",
    "    IDlist = [i.text for i in result.find_all('Id')]\n",
    "    num_papers = len(IDlist)\n",
    "    if num_papers > 0:\n",
    "        new_has_papers.add(author)\n",
    "    else:\n",
    "        new_no_papers.append(author)\n",
    "\n",
    "# Step 2: Put together final author list\n",
    "\n",
    "# Combine and include authors who didn't have papers with LastFirst but also don't have a MI\n",
    "author_set = has_papers.union(new_has_papers)\n",
    "authors_list = list(author_set) + ['\"Sorri Y\"[au]', '\"Connolly A\"[au]', '\"Ghassemieh B\"[au]']\n",
    "authors = '(' + ' OR '.join(authors_list) + ')'  # Make authors into string\n",
    "\n",
    "#############\n",
    "# Do search #\n",
    "#############\n",
    "\n",
    "print('Doing PubMed search')\n",
    "\n",
    "date_range = start_date + ':' + end_date + '[pdat]'\n",
    "url = prefix + date_range + '+AND+' + affiliations + '+AND+' + authors + '+AND+' + concepts\n",
    "page = requests.get(url).text\n",
    "result = BeautifulSoup(page, 'xml')\n",
    "IDlist = [i.text for i in result.find_all('Id')]\n",
    "\n",
    "out = ';'.join(list(IDlist))\n",
    "\n",
    "print('Done!\\n')\n",
    "print(f'PubMed IDs of papers from SEATRAC authors published between {start_date} and {end_date}:\\n')\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
